{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e730799c",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence (BSc)\n",
    "## Week 2 — What is AI? What is an Agent? (AIMA Ch. 2)\n",
    "\n",
    "Name: Mukhammadsaiid Norbaev\n",
    "\n",
    "Date of last update: 08/02/2026\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561335f",
   "metadata": {},
   "source": [
    "### Today’s goals\n",
    "By the end of this notebook you should be able to:\n",
    "- Explain what an **agent** is (in AI terms).\n",
    "- Describe an **environment** and its key properties.\n",
    "- Define **rationality** using performance measures and constraints.\n",
    "- Implement and explain a very simple **reflex agent**.\n",
    "- Practise **explainability**: explain *why* your agent acts the way it does.\n",
    "\n",
    "### How to use this notebook\n",
    "- Read the markdown cells first.\n",
    "- Run code cells in order.\n",
    "- Fill in the **TODO** sections.\n",
    "- Answer the reflection questions in **your own words**.\n",
    "\n",
    "### Reading\n",
    "- Russell & Norvig (AIMA), Chapter 2: Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d35de",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Run this cell first. If something errors, ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9e5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dab30",
   "metadata": {},
   "source": [
    "## 1. Concepts: Agent, Environment, Percepts, Actions\n",
    "\n",
    "In AIMA, an **agent** is anything that:\n",
    "- **perceives** its environment (gets percepts)\n",
    "- **acts** in the environment (takes actions)\n",
    "\n",
    "A simple picture:\n",
    "\n",
    "**Environment → Percepts → Agent → Actions → Environment**\n",
    "\n",
    "### Quick check (write your answers)\n",
    "**Q1:** Is a calculator an agent? Why or why not?\n",
    "\n",
    "**Q2:** Is a thermostat an agent? Why or why not?\n",
    "\n",
    "Write answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57befd92",
   "metadata": {},
   "source": [
    "### Your answers\n",
    "- Q1: yes, it is. as it acts within an environment in case of a caclulator it would be numbers and maths operators. it takes data and processes and gives the result, everything within an environment\n",
    "- Q2: yes, it could also be considered an agent, environment in this case would be the temperature bar, and it takes percepts, which is it checking temperature and it takes action changing the its environment, the temperature bar accordingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976189c",
   "metadata": {},
   "source": [
    "## 2. A Tiny Environment: 2×2 Vacuum World\n",
    "\n",
    "We will use a very small **grid environment**:\n",
    "- The agent is on one square.\n",
    "- Each square is either **dirty** or **clean**.\n",
    "- The agent can:\n",
    "  - move up/down/left/right\n",
    "  - clean (\"SUCK\")\n",
    "\n",
    "### Why this environment?\n",
    "It is small enough to understand *every step* and still illustrates real AI ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial world:\n",
      "AC .D\n",
      ".D .D\n",
      "\n",
      "{(0, 0): False, (0, 1): True, (1, 0): True, (1, 1): True}\n",
      "Initial world:\n",
      "\n",
      "(0, 0): A  |  CLEAN (False)\n",
      "(0, 1): .  |  DIRTY (True)\n",
      "\n",
      "(1, 0): .  |  DIRTY (True)\n",
      "(1, 1): .  |  DIRTY (True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment settings\n",
    "ROWS = 2\n",
    "COLS = 2\n",
    "\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT', 'SUCK']\n",
    "\n",
    "# We represent the world as a dictionary:\n",
    "# world[(r, c)] = True means DIRTY\n",
    "# world[(r, c)] = False means CLEAN\n",
    "\n",
    "def make_random_world(rows: int, cols: int, dirt_prob: float = 0.7) -> Dict[Tuple[int,int], bool]:\n",
    "    # dirt_prob - controls the probability that each cell starts dirty\n",
    "    world = {}\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            world[(r, c)] = (random.random() < dirt_prob)\n",
    "            # random.random() returns a float between 0.0 and 1.0\n",
    "            # and the value will either be True (if float is < 0.7) or False (if float is > 0.7)\n",
    "    return world\n",
    "\n",
    "def print_world(world: Dict[Tuple[int,int], bool], agent_pos: Tuple[int,int], rows: int, cols: int) -> None:\n",
    "    # Simple text display\n",
    "    for r in range(rows):\n",
    "        row_cells = []\n",
    "        for c in range(cols): # column iteration; left to right\n",
    "            is_dirty = world[(r, c)] # boolean, checks whether value at this key is true(dirty) or false(clean)\n",
    "            if (r, c) == agent_pos: # agent position check \n",
    "                cell = 'A'  # agent is here\n",
    "            else:\n",
    "                cell = '.' # if agent is not here\n",
    "            cell += 'D' if is_dirty else 'C' # adding dirt status ex: if A and is_dirty => \"AD\", else \".C\"\n",
    "            row_cells.append(cell) # adds the finished cell string\n",
    "        print(' '.join(row_cells))\n",
    "        # prints one row, space-seperated\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_world_verbose(world, agent_pos, rows, cols):\n",
    "    # print the same thing but different looking\n",
    "    print(\"Initial world:\\n\")\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            is_dirty = world[(r, c)]\n",
    "            agent = \"A\" if (r, c) == agent_pos else \".\"\n",
    "            state = \"DIRTY\" if is_dirty else \"CLEAN\"\n",
    "            print(f\"({r}, {c}): {agent}  |  {state} ({is_dirty})\")\n",
    "        print()\n",
    "\n",
    "world = make_random_world(ROWS, COLS, dirt_prob=0.7)\n",
    "agent_pos = (0, 0)\n",
    "\n",
    "print('Initial world:')\n",
    "print_world(world, agent_pos, ROWS, COLS)\n",
    "\n",
    "print(world)\n",
    "\n",
    "print_world_verbose(world, agent_pos, ROWS, COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004af6df",
   "metadata": {},
   "source": [
    "## 3. Separating Environment from Agent\n",
    "\n",
    "For explainability, we will separate:\n",
    "- **Sense** (environment → percept)\n",
    "- **Agent** (percept → action)\n",
    "- **Act** (environment + action → new environment)\n",
    "\n",
    "This separation helps you explain:\n",
    "- what the agent knows\n",
    "- what the agent decides\n",
    "- how the environment changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e80a762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example percept: {'pos': (0, 0), 'is_dirty_here': False}\n"
     ]
    }
   ],
   "source": [
    "def sense(world: Dict[Tuple[int,int], bool], agent_pos: Tuple[int,int]) -> Dict[str, object]:\n",
    "    \"\"\"Return the percept. Here: location and whether current square is dirty.\"\"\"\n",
    "    # the point here is separation of concerns, core ai concept\n",
    "    # agent receives only where it is and whether its current tile is dirty, not the full world\n",
    "    \"\"\"in ai, an agent:\n",
    "    does not have access to world\n",
    "    only receives a percept from the environment\n",
    "    this function simulates that boundary\n",
    "    sense() is like the contract that limits what the agent is allowed to know about the world.\"\"\"\n",
    "    r, c = agent_pos\n",
    "    percept = {\n",
    "        'pos': agent_pos,\n",
    "        'is_dirty_here': world[(r, c)]\n",
    "    }\n",
    "    return percept\n",
    "\n",
    "def act(world: Dict[Tuple[int,int], bool], agent_pos: Tuple[int,int], action: str, rows: int, cols: int) -> Tuple[Dict[Tuple[int,int], bool], Tuple[int,int]]:\n",
    "    \"\"\"Apply the action to the environment. Returns (new_world, new_agent_pos).\"\"\"\n",
    "    r, c = agent_pos\n",
    "    new_world = dict(world)  # copy\n",
    "    new_pos = agent_pos\n",
    "\n",
    "    if action == 'SUCK':\n",
    "        # Clean the current square\n",
    "        new_world[(r, c)] = False # false means clean\n",
    "        return new_world, new_pos # changed world is returned, given tile has been cleaned\n",
    "\n",
    "    if action == 'UP':\n",
    "        if r > 0: # this checks whether it is possible to go up\n",
    "            new_pos = (r - 1, c)\n",
    "    elif action == 'DOWN':\n",
    "        if r < rows - 1:\n",
    "            new_pos = (r + 1, c)\n",
    "    elif action == 'LEFT':\n",
    "        if c > 0:\n",
    "            new_pos = (r, c - 1)\n",
    "    elif action == 'RIGHT':\n",
    "        if c < cols - 1:\n",
    "            new_pos = (r, c + 1)\n",
    "\n",
    "    return new_world, new_pos\n",
    "\n",
    "percept = sense(world, agent_pos)\n",
    "print('Example percept:', percept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d5044",
   "metadata": {},
   "source": [
    "## 4. A Simple Reflex Agent\n",
    "\n",
    "A reflex agent uses **if–else rules**.\n",
    "\n",
    "### Reflex rule (very simple)\n",
    "- If current square is dirty → SUCK\n",
    "- Otherwise → move randomly\n",
    "\n",
    "This is not “smart”, but it is a valid agent.\n",
    "\n",
    "### TODO\n",
    "Read the function and make sure you can explain it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c244e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If dirty -> SUCK\n",
      "If clean -> UP\n"
     ]
    }
   ],
   "source": [
    "def reflex_agent(percept: Dict[str, object]) -> str:\n",
    "    # percept: Dict[str, object] -- this is a type contract that says:\n",
    "    # keys must be str\n",
    "    # values must be instances of object\n",
    "    \"\"\"the reflex agent does not care about pos, types; it only cares that \" is_dirty_here\" exists, it is true or false\"\"\"\n",
    "    if percept['is_dirty_here']:\n",
    "        return 'SUCK'\n",
    "    else:\n",
    "        return random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "\n",
    "# Test the agent decision once\n",
    "test_percept = {'pos': (0,0), 'is_dirty_here': True}\n",
    "print('If dirty ->', reflex_agent(test_percept))\n",
    "test_percept = {'pos': (0,0), 'is_dirty_here': False}\n",
    "print('If clean ->', reflex_agent(test_percept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44006b41",
   "metadata": {},
   "source": [
    "## 5. Running a Simulation\n",
    "\n",
    "We will run the agent for a number of steps.\n",
    "\n",
    "### Performance measure\n",
    "We need a way to say if the agent is doing well.\n",
    "\n",
    "For now:\n",
    "- **+1** point for each clean square at each time step\n",
    "\n",
    "This means the agent is rewarded for keeping the world clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2ce1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation...\n",
      "AD .D\n",
      ".D .D\n",
      "\n",
      "Time 0: action=SUCK, score_this_step=1\n",
      "AC .D\n",
      ".D .D\n",
      "\n",
      "Time 1: action=DOWN, score_this_step=1\n",
      ".C .D\n",
      "AD .D\n",
      "\n",
      "Time 2: action=SUCK, score_this_step=2\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 3: action=LEFT, score_this_step=2\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 4: action=UP, score_this_step=2\n",
      "AC .D\n",
      ".C .D\n",
      "\n",
      "Time 5: action=DOWN, score_this_step=2\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 6: action=DOWN, score_this_step=2\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 7: action=DOWN, score_this_step=2\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Total score: 14\n"
     ]
    }
   ],
   "source": [
    "def performance(world: Dict[Tuple[int,int], bool], rows: int, cols: int) -> int:\n",
    "    # world contains all the tiles with bool value \n",
    "    # Count clean squares\n",
    "    clean = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if world[(r, c)] == False:\n",
    "                clean += 1\n",
    "    return clean\n",
    "\n",
    "def run_simulation(agent_fn, steps: int = 10, rows: int = 2, cols: int = 2, dirt_prob: float = 0.7, show: bool = True):\n",
    "    world = make_random_world(rows, cols, dirt_prob)\n",
    "    agent_pos = (0, 0)\n",
    "    total_score = 0\n",
    "\n",
    "    if show:\n",
    "        print('Starting simulation...')\n",
    "        print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "    for t in range(steps):\n",
    "        percept = sense(world, agent_pos) # this returns dict[]{\n",
    "                                                                        #   'pos': agent_pos,\n",
    "                                                                        #   'is_dirty_here': world[(r, c)]\n",
    "                                                                        # }\n",
    "        \"\"\"percept extracts only local information, where the agent is, not the whole world, so sense only return one dict with two elements\n",
    "        pos: value and is dirty : value\n",
    "        in this case if first iteration it is as agent pos (0, 0) it will return {pos: (0, 0) and is_dirty_here: True or False}\n",
    "        percept is what the agent senses at its feet not a map of the hosue\"\"\"\n",
    "        action = agent_fn(percept) # agent_fn is a placeholder name for whatever agent you pass in, in our case it is reflex_agent\n",
    "        \"\"\"action has whatever the agent passes, in here, reflex agent returns str either SUCK or random move UP, DOWN, LEFT, RIGHT\"\"\"\n",
    "        world, agent_pos = act(world, agent_pos, action, rows, cols) # acts: moves, cleans => returns new world and agent pos\n",
    "\n",
    "        score_t = performance(world, rows, cols) # this line has number of clean tiles from the changed world\n",
    "        total_score += score_t # total_score accumulates the total number of clean tiles over time (state-based reward, not per-action)\n",
    "        \"\"\"total_score = 1 + 2 + 2 + 3 + 4 = 12\n",
    "            ths is what it could look like, this is not counting clean actions\n",
    "            it is summing state quality over time, summing the whole state\"\"\"\n",
    "\n",
    "\n",
    "        if show:\n",
    "            print(f'Time {t}: action={action}, score_this_step={score_t}') # i am not sure why we need flag show, it is alwayws true, we are not chagning it\n",
    "            # the output would be the same even without show i think\n",
    "            print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "    return total_score\n",
    "\n",
    "score = run_simulation(reflex_agent, steps=8, rows=ROWS, cols=COLS, dirt_prob=0.7, show=True)\n",
    "print('Total score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8686c08",
   "metadata": {},
   "source": [
    "## 6. Explainability Task (Important)\n",
    "\n",
    "Answer in your own words:\n",
    "\n",
    "1. What information does the agent use to decide? \n",
    "2. Why does the agent sometimes move \"badly\"?\n",
    "3. What is the agent trying to maximise in this environment?\n",
    "\n",
    "Write answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca2960",
   "metadata": {},
   "source": [
    "### Your answers\n",
    "- Q1: it uses the percept provided by the environment, it has the location and info clean/dirty, the agent can't see the whole world, only percept\n",
    "- Q2: it may go to the location, which is clean, where nothing is required to do, in this case resources are wasted\n",
    "- Q3: performance measure, number of clean tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17beb4d",
   "metadata": {},
   "source": [
    "## 7. Rationality Depends on the Performance Measure\n",
    "\n",
    "Let’s change what we mean by “good”.\n",
    "\n",
    "### New performance measure\n",
    "- Clean squares are good\n",
    "- BUT moving costs energy\n",
    "\n",
    "We will implement:\n",
    "- +1 for each clean square per step\n",
    "- -1 for every move action\n",
    "\n",
    "### TODO\n",
    "Complete the function `performance_with_energy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f10409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation (energy cost)...\n",
      "AD .D\n",
      ".C .D\n",
      "\n",
      "Time 0: action=SUCK, score_this_step=2\n",
      "AC .D\n",
      ".C .D\n",
      "\n",
      "Time 1: action=DOWN, score_this_step=1\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 2: action=LEFT, score_this_step=1\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 3: action=UP, score_this_step=1\n",
      "AC .D\n",
      ".C .D\n",
      "\n",
      "Time 4: action=DOWN, score_this_step=1\n",
      ".C .D\n",
      "AC .D\n",
      "\n",
      "Time 5: action=UP, score_this_step=1\n",
      "AC .D\n",
      ".C .D\n",
      "\n",
      "Time 6: action=LEFT, score_this_step=1\n",
      "AC .D\n",
      ".C .D\n",
      "\n",
      "Time 7: action=RIGHT, score_this_step=1\n",
      ".C AD\n",
      ".C .D\n",
      "\n",
      "Total score (energy): 9\n"
     ]
    }
   ],
   "source": [
    "def performance_with_energy(world: Dict[Tuple[int,int], bool], rows: int, cols: int, last_action: str) -> int:\n",
    "    # TODO: start from clean squares score\n",
    "    clean_score = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if world[(r, c)] == False:\n",
    "                clean_score += 1\n",
    "\n",
    "    # TODO: subtract 1 for move actions (UP/DOWN/LEFT/RIGHT)\n",
    "    move_penalty = 0\n",
    "    if last_action in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "        move_penalty = 1\n",
    "\n",
    "    return clean_score - move_penalty\n",
    "\n",
    "def run_simulation_energy(agent_fn, steps: int = 10, rows: int = 2, cols: int = 2, dirt_prob: float = 0.7, show: bool = True):\n",
    "    world = make_random_world(rows, cols, dirt_prob)\n",
    "    agent_pos = (0, 0)\n",
    "    total_score = 0\n",
    "\n",
    "    if show:\n",
    "        print('Starting simulation (energy cost)...')\n",
    "        print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "    for t in range(steps):\n",
    "        percept = sense(world, agent_pos) # returns dict {pos:x,y; isdirtyhere:true/false}\n",
    "        action = agent_fn(percept) # returns action, suck or move\n",
    "        world, agent_pos = act(world, agent_pos, action, rows, cols) # acts/ cleans, returns new world\n",
    "\n",
    "        score_t = performance_with_energy(world, rows, cols, action) # returns clean_score - move_penalty of the new world\n",
    "        total_score += score_t # adds the performance score to total which is the difference of clean-socre and move penalty\n",
    "\n",
    "        if show:\n",
    "            print(f'Time {t}: action={action}, score_this_step={score_t}')\n",
    "            print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "    return total_score\n",
    "\n",
    "score2 = run_simulation_energy(reflex_agent, steps=8, rows=ROWS, cols=COLS, dirt_prob=0.7, show=True)\n",
    "print('Total score (energy):', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d7fba",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "1. Did the agent’s behaviour change? Why or why not?\n",
    "2. Is the reflex agent rational under this new performance measure?\n",
    "\n",
    "Write answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2247ee1",
   "metadata": {},
   "source": [
    "### Your answers\n",
    "- Q1: no it didn't, we are just penalyzing for moves and substracting it from cleanliness score, the agent is behaving the same way, it is not aware of the energy penalty\n",
    "- Q2: no, it is the same, it does not maximise the performance measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7006bc",
   "metadata": {},
   "source": [
    "## 8. Environment Types (Light Practice)\n",
    "\n",
    "AIMA describes environment properties such as:\n",
    "- fully observable vs partially observable\n",
    "- deterministic vs stochastic\n",
    "- episodic vs sequential\n",
    "- static vs dynamic\n",
    "\n",
    "### Task\n",
    "Classify each environment (write short answers):\n",
    "1. This vacuum world\n",
    "2. Chess\n",
    "3. Driving in London\n",
    "4. A recommendation system (Netflix/YouTube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc83d8",
   "metadata": {},
   "source": [
    "## Fully observable vs partially observable\n",
    "\n",
    "- Fully observable: agent can see the complete state relevant to decision-making\n",
    "- Partially observable: agent cannot see everything (hidden info, noise, uncertainty)\n",
    "\n",
    "## Deterministic vs stochastic\n",
    "\n",
    "- Deterministic: next state is completely determined by current state + action\n",
    "- Stochastic: outcomes involve randomness or uncertainty\n",
    "- This is not about “complexity” — it’s about uncertainty in outcomes.\n",
    "\n",
    "## Episodic vs sequential\n",
    "\n",
    "- Episodic: each decision is independent of previous ones\n",
    "- Sequential: current actions affect future states and decisions\n",
    "\n",
    "## Static vs dynamic\n",
    "\n",
    "- Static: environment does not change while the agent is deciding\n",
    "- Dynamic: environment can change independently of the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392cf50",
   "metadata": {},
   "source": [
    "### Your answers\n",
    "1. Vacuum world: partially observable, stochastic, episodic, static\n",
    "2. Chess: fully observable (before moving it has to know where the other figures are), determistic, sequental, static\n",
    "3. Driving in London: partially observable, stochastic, sequential, dynamic\n",
    "4. Recommendation system: partially observable, stochastic, sequential, dynamic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c5f6",
   "metadata": {},
   "source": [
    "## 9. Optional Challenge (For fast finishers)\n",
    "\n",
    "### Challenge A: Better Reflex Agent\n",
    "Change the agent so that when the current square is clean it prefers to move toward a dirty square.\n",
    "\n",
    "Hints:\n",
    "- You will need to give the agent more information (more percepts).\n",
    "- For example, sense *adjacent squares*.\n",
    "\n",
    "### Challenge B: Bigger world\n",
    "Try a 3×3 grid and see how performance changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becddb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation (energy cost)...\n",
      "AC .C\n",
      ".C .D\n",
      "\n",
      "Time 0: action=UP, score_this_step=2\n",
      "AC .C\n",
      ".C .D\n",
      "\n",
      "Time 1: action=DOWN, score_this_step=2\n",
      ".C .C\n",
      "AC .D\n",
      "\n",
      "Time 2: action=RIGHT, score_this_step=2\n",
      ".C .C\n",
      ".C AD\n",
      "\n",
      "Time 3: action=SUCK, score_this_step=4\n",
      ".C .C\n",
      ".C AC\n",
      "\n",
      "all tiles clean at time 3. Ending simulation eaerly.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 9. Optional Challenge (For fast finishers)\n",
    "\n",
    "# Challenge A: Better Reflex Agent\n",
    "# Change the agent so that when the current square is clean it prefers to move toward a dirty square.\n",
    "\n",
    "# Hints:\n",
    "# - You will need to give the agent more information (more percepts).\n",
    "# - For example, sense *adjacent squares*.\n",
    "\n",
    "def sense_with_neighbors(world: Dict[Tuple[int,int], bool],\n",
    "                         agent_pos: Tuple[int,int],\n",
    "                         rows: int,\n",
    "                         cols: int) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Returns a percept that includes:\n",
    "    - agent position\n",
    "    - whether current square is dirty\n",
    "    - dirt status of adjacent squares\n",
    "    \"\"\"\n",
    "\n",
    "    r, c = agent_pos\n",
    "\n",
    "    percept = {\n",
    "        'pos': agent_pos,\n",
    "        'is_dirty_here': world[(r, c)],\n",
    "        'neighbors': {}\n",
    "    }\n",
    "\n",
    "    # Check each direction safely (stay inside grid)\n",
    "    if r > 0:\n",
    "        percept['neighbors']['UP'] = world[(r - 1, c)]\n",
    "    if r < rows - 1:\n",
    "        percept['neighbors']['DOWN'] = world[(r + 1, c)]\n",
    "    if c > 0:\n",
    "        percept['neighbors']['LEFT'] = world[(r, c - 1)]\n",
    "    if c < cols - 1:\n",
    "        percept['neighbors']['RIGHT'] = world[(r, c + 1)]\n",
    "\n",
    "    return percept\n",
    "\n",
    "# percept example:\n",
    "# {\n",
    "#   'pos': (0, 0),\n",
    "#   'is_dirty_here': False,\n",
    "#   'neighbors': {\n",
    "#       'DOWN': True,\n",
    "#       'RIGHT': False\n",
    "#   }\n",
    "# }\n",
    "\n",
    "def better_reflex_agent(percept: Dict[str, object]) -> str:\n",
    "    \"\"\"\n",
    "    Improved reflex agent:\n",
    "    - If current square is dirty → SUCK\n",
    "    - Else, move toward a dirty neighboring square if one exists\n",
    "    - Else, move randomly\n",
    "    \"\"\"\n",
    "\n",
    "    # If current tile is dirty, clean it\n",
    "    if percept['is_dirty_here']:\n",
    "        return 'SUCK'\n",
    "\n",
    "    # Look at neighboring tiles\n",
    "    neighbors = percept['neighbors']\n",
    "\n",
    "    # Prefer moving toward a dirty neighbor\n",
    "    for direction, is_dirty in neighbors.items():\n",
    "        if is_dirty:\n",
    "            return direction\n",
    "\n",
    "    # If no dirty neighbors, move randomly\n",
    "    return random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "\n",
    "\n",
    "def run_simulation_energy(agent_fn, steps: int = 10, rows: int = 2, cols: int = 2, dirt_prob: float = 0.7, show: bool = True):\n",
    "    world = make_random_world(rows, cols, dirt_prob)\n",
    "    agent_pos = (0, 0)\n",
    "    total_score = 0\n",
    "\n",
    "    if show:\n",
    "        print('Starting simulation (energy cost)...')\n",
    "        print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "    for t in range(steps):\n",
    "        percept = sense_with_neighbors(world, agent_pos, rows, cols) # returns dict {pos:x,y; isdirtyhere:true/false}\n",
    "        action = agent_fn(percept) # returns action, suck or move\n",
    "        world, agent_pos = act(world, agent_pos, action, rows, cols) # acts/ cleans, returns new world\n",
    "\n",
    "        score_t = performance_with_energy(world, rows, cols, action) # returns clean_score - move_penalty of the new world\n",
    "        total_score += score_t # adds the performance score to total which is the difference of clean-socre and move penalty\n",
    "\n",
    "        if show:\n",
    "            print(f'Time {t}: action={action}, score_this_step={score_t}')\n",
    "            print_world(world, agent_pos, rows, cols)\n",
    "\n",
    "        all_clean = all(not dirty for dirty in world.values())\n",
    "        if all_clean:\n",
    "            if show:\n",
    "                print(f\"all tiles clean at time {t}. Ending simulation eaerly.\")\n",
    "            break\n",
    "\n",
    "    return total_score\n",
    "\n",
    "# percept = sense_with_neighbors(world, agent_pos, rows, cols)\n",
    "# action = agent_fn(percept)\n",
    "\n",
    "score = run_simulation_energy(\n",
    "    better_reflex_agent,\n",
    "    steps=8,\n",
    "    rows=ROWS,\n",
    "    cols=COLS,\n",
    "    dirt_prob=0.7,\n",
    "    show=True\n",
    ")\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071cbe4",
   "metadata": {},
   "source": [
    "## 10. Exit Ticket (The things we need to know by today)\n",
    "\n",
    "Answer briefly:\n",
    "1. In one sentence, what is an **agent**?\n",
    "2. In one sentence, what does **rational** mean in AI?\n",
    "3. Name one thing that makes real environments harder than this vacuum world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6355c",
   "metadata": {},
   "source": [
    "### Your exit ticket\n",
    "1. agent is an entity that perceives its environment through sensors and acts upon that environment through actions\n",
    "2. making the best possible solution considering the environemnt, that maximises performance measure\n",
    "3. dynamic changes, partial observability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
